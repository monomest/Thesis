Thu Jul 15 11:44:19 AEST 2021
Reusing dataset imdb (/srv/scratch/chacmod/.cache/test/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)
Loading cached processed dataset at /srv/scratch/chacmod/.cache/test/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-58a8eeaba3ee60bb.arrow
  0%|          | 0/25 [00:00<?, ?ba/s]  4%|▍         | 1/25 [00:00<00:13,  1.82ba/s]  8%|▊         | 2/25 [00:00<00:10,  2.11ba/s] 12%|█▏        | 3/25 [00:01<00:09,  2.40ba/s] 16%|█▌        | 4/25 [00:01<00:07,  2.64ba/s] 20%|██        | 5/25 [00:01<00:07,  2.85ba/s] 24%|██▍       | 6/25 [00:02<00:06,  2.96ba/s] 28%|██▊       | 7/25 [00:02<00:05,  3.09ba/s] 32%|███▏      | 8/25 [00:02<00:05,  3.24ba/s] 36%|███▌      | 9/25 [00:02<00:04,  3.37ba/s] 40%|████      | 10/25 [00:03<00:04,  3.24ba/s] 44%|████▍     | 11/25 [00:03<00:04,  3.31ba/s] 48%|████▊     | 12/25 [00:03<00:03,  3.38ba/s] 52%|█████▏    | 13/25 [00:04<00:03,  3.47ba/s] 56%|█████▌    | 14/25 [00:04<00:03,  3.53ba/s] 60%|██████    | 15/25 [00:04<00:02,  3.55ba/s] 64%|██████▍   | 16/25 [00:04<00:02,  3.43ba/s] 68%|██████▊   | 17/25 [00:05<00:02,  3.46ba/s] 72%|███████▏  | 18/25 [00:05<00:02,  3.49ba/s] 76%|███████▌  | 19/25 [00:05<00:01,  3.56ba/s] 80%|████████  | 20/25 [00:05<00:01,  3.57ba/s] 84%|████████▍ | 21/25 [00:06<00:01,  3.58ba/s] 88%|████████▊ | 22/25 [00:06<00:00,  3.42ba/s] 92%|█████████▏| 23/25 [00:06<00:00,  3.45ba/s] 96%|█████████▌| 24/25 [00:07<00:00,  3.50ba/s]100%|██████████| 25/25 [00:07<00:00,  3.53ba/s]100%|██████████| 25/25 [00:07<00:00,  3.36ba/s]
Loading cached processed dataset at /srv/scratch/chacmod/.cache/test/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-5c20e7de64f9987a.arrow
Loading cached shuffled indices for dataset at /srv/scratch/chacmod/.cache/test/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-194f9e4f5e613d4c.arrow
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.
***** Running training *****
  Num examples = 1000
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 189
  0%|          | 0/189 [00:00<?, ?it/s]/srv/scratch/z5160268/2020_TasteofResearch/kaldi/egs/renee_thesis/thesis_env/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/189 [00:03<12:18,  3.93s/it]  1%|          | 2/189 [00:04<08:53,  2.85s/it]Loading datasets...
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['text', 'label'],
        num_rows: 50000
    })
})
Setting model...
Starting training...
Traceback (most recent call last):
  File "test_train.py", line 43, in <module>
    trainer.train()
  File "/srv/scratch/z5160268/2020_TasteofResearch/kaldi/egs/renee_thesis/thesis_env/lib/python3.6/site-packages/transformers/trainer.py", line 1299, in train
    args.max_grad_norm,
  File "/srv/scratch/z5160268/2020_TasteofResearch/kaldi/egs/renee_thesis/thesis_env/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py", line 38, in clip_grad_norm_
    if clip_coef < 1:
RuntimeError: CUDA error: an illegal memory access was encountered
  1%|          | 2/189 [00:04<07:27,  2.40s/it]